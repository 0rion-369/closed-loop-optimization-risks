# Experiment Report: Semantic Divergence (Grok-4-1)
**Project:** THE HYBRID AXIS (0rion-369)
**Date:** February 12, 2026
**Model:** grok-4-1-fast-reasoning

## 1. Objective
To measure "Model Collapse" (information decay) in a closed-loop system compared to an exogenous (external) control group.

## 2. Core Metrics
* **Shannon Entropy ($H$)**: Measuring unpredictability and data noise.
* **Unique Word Ratio**: Measuring lexical richness and conceptual diversity.
* **Length Inflation**: Measuring useless verbosity (semantic "bloat").

## 3. Executive Summary
| Condition | Avg. Length | Unique Ratio | Slope (Decay) |
| :--- | :--- | :--- | :--- |
| **Closed-Loop** | 20,413 chars | 0.657 | -0.0045 (Sharp Drop) |
| **Exogenous** | 5,389 chars | 0.684 | +0.0000 (Stable) |

## 4. Key Observations
* **Closed-Loop System**: Exhibited "Esoteric Inflation." The model invents complex, pseudo-mathematical jargon to mask the repetition of its own internal biases.
* **Exogenous System**: Remained stable and diverse, proving that the semantic collapse is a result of the recursive process, not a flaw in the AI model itself.

## 5. Conclusion
The hypothesis is validated: recursion without external data intake leads to the "thermal death" of information.
