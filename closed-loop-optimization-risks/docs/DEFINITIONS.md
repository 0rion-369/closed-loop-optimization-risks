# Definitions

This document defines technical terms used in the Closed-Loop Optimization Risk Framework.

---

## Closed-Loop Optimization

An optimization regime in which training data, evaluation signals, and corrective feedback are primarily generated by the system itself or by closely related systems.

---

## Self-Referential Learning

A learning process in which model updates depend largely on internally generated representations, simulations, or predictions.

---

## Exploration Collapse

A dynamic condition in which a learning system:
- reduces exploratory behavior,
- converges toward stable internal attractors,
- and exhibits diminishing epistemic returns over time.

---

## Epistemic Stagnation

A long-horizon state in which learning persists nominally but fails to produce structurally novel representations or behaviors.

---

## Endogenous Variance

Variance produced by a system’s internal stochasticity, sampling, or recombination of known patterns.

Endogenous variance is typically compressible.

---

## Exogenous Variance

Variance originating from sources not fully captured by the system’s internal models or training distributions.

Exogenous variance is not fully derivable.

---

## Informational Redundancy

A condition in which new data provides minimal incremental information relative to existing internal representations.

---

## Optimization Pressure

The influence exerted by loss functions, reward signals, or objective metrics that bias learning toward predictability and stability.

---

## Stable Attractor

A region of the system’s parameter or behavior space toward which optimization dynamics naturally converge.

---

## Long-Horizon Risk

A failure mode that emerges only after extended operation and is not detectable via short-term evaluation metrics.

---

## Synthetic Data Dependence

Reliance on artificially generated data for training or evaluation, especially when such data is produced by the system itself or closely related models.

---

End of definitions.
